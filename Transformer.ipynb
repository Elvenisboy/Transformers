{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer for Language Translation implementation in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will go trough an PyTorch implementation of transformer based on [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) paper.\n",
    "\n",
    "In [another notebook](https://github.com/mf1024/transformers) I implemented Transforer Encoder model and trained it for Language Modeling task. \n",
    "\n",
    "In this notebook I will continue and add the Decoder component to the implementation and train the whole on Language Translation task.\n",
    "\n",
    "Troughout this notebook as well I **use some of the illustrations from the incredible [Illustrated transformer](https://jalammar.github.io/illustrated-transformer) blog post by *Jay Alammar***. I highly recommend you to take a look at it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For the translation task I will use a small english-french [dataset](https://raw.githubusercontent.com/mf1024/transformers/master/fra-eng/fra.txt). The dataset consists of just 170k sentences but that is enough to train and demonstrate the Transformer.\n",
    "\n",
    "I will use the same PyTorch [Dataset](https://github.com/mf1024/transformers/blob/master/fra_eng_dataset.py) implementation to process the sentences and prepare the batches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer architecture:\n",
    "\n",
    "<img src=\"imgs/transformer.png\">\n",
    "\n",
    "The key idea of Transformer is to avoid using [reccurence](https://arxiv.org/abs/1409.3215) at all for encoding and decoding variable-length sequences. That solves issues with long-range dependencies and the amount of computation that can be parallelized.\n",
    "\n",
    "The transformer consists of two parts - the Encoder and the Decoder. They are both very similar in their structure but different in a few aspects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder and Decoder\n",
    "\n",
    "<img src=\"imgs/encoder-decoder.png\">\n",
    "\n",
    "In the full Transformer implementation, first, the sentence in the source language is encoded. One the encoding is done, the output of the last encoder layer is used in every layer of decoder, where the last layer generates the sentence in target language. \n",
    "\n",
    "\n",
    "<img src=\"imgs/transformer_decoding.gif\">\n",
    "\n",
    "The Decoder layer consists of same sublayers as Encoder(Self Attention and Feed Forward) with one additional sublayer - Memory Attention. The Memory Attention sublayer is similar to Self Attention except it uses **Key** and **Value** matrices from the Encoder.\n",
    "\n",
    "The Decoder will use these **Key** and **Value** matrices but will learn to calculate it's own **Query** to gather the information from the encoded sentences.\n",
    "\n",
    "When decoding the sentence, each word is prediced one at a time, meaning that for Decoder to predict the word x(i) the input of the Decoder should be all the words in target langugae from 1 to i-1 . During the traing the decoding can be paralelized using masking, but during translation it is going to take as many Decoder runs as there are words in the target sentence as shown in the GIF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from fra_eng_dataset import FraEngDataset, fra_eng_dataset_collate\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.K = nn.Linear(d_model, d_model)\n",
    "        self.V = nn.Linear(d_model, d_model)\n",
    "        self.Q = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, padding_mask = None, subsq_mask = None):\n",
    "        # x shape: [N, SEQ, D_MODEL]\n",
    "\n",
    "        keys = self.K.forward(x)\n",
    "        values = self.V.forward(x)\n",
    "        queries = self.Q.forward(x)\n",
    "\n",
    "        sqrt_d = self.d_model ** 0.5\n",
    "\n",
    "        att = torch.matmul(queries, keys.transpose(1,2)) / sqrt_d\n",
    "        # att shape: [N, SEQ, SEQ]\n",
    "        # Broadcast padding mask to word attentions so that word attention does not attend to positions outside the sentence\n",
    "        if padding_mask is not None:\n",
    "            att = att + padding_mask.transpose(1,2)\n",
    "        # Add subsequent mask so that each position can attend only itself and the previous elements\n",
    "        if subsq_mask is not None:\n",
    "            att = att + subsq_mask.unsqueeze(0)\n",
    "\n",
    "        att_softmax = torch.softmax(att, dim=2)\n",
    "        # shape: [N, SEQ, SEQ]\n",
    "        att_out = torch.matmul(att_softmax, values)\n",
    "        # shape: [N, SEQ, D_MODEL]\n",
    "\n",
    "        return att_out, keys, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Attention\n",
    "Implementation is the same as Self Attention only the keys and values matrices are not calculated from weights and input but are passed to the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemAttentionHead(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.Q = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mem_padding_mask, keys = None, values = None):\n",
    "\n",
    "        # X shape: [N, SEQ, D_MODEL]\n",
    "        queries = self.Q.forward(x)\n",
    "        sqrt_d = self.d_model ** 0.5\n",
    "\n",
    "        att = torch.matmul(queries, keys.transpose(1,2)) / sqrt_d\n",
    "        # att shape: [N, SEQ_TGT, SEQ_SRC]\n",
    "\n",
    "        # Broadcast padding mask to word attentions so that word attention does not attend to positions outside the source sentence\n",
    "        if mem_padding_mask is not None:\n",
    "            att = att + mem_padding_mask.transpose(1,2)\n",
    "\n",
    "        att_softmax = torch.softmax(att, dim=2)\n",
    "        # shape: [N, SEQ_TGT, SEQ_SRC]\n",
    "        att_out = torch.matmul(att_softmax, values)\n",
    "        # shape: [N, SEQ_TGT, D_MODEL]\n",
    "\n",
    "        return att_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attentions for Self Attention and Memory Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.heads = nn.ModuleList([SelfAttentionHead(d_model) for i in range(num_heads)])\n",
    "        self.linear = nn.Linear(num_heads * d_model, d_model)\n",
    "\n",
    "    def forward(self, src, src_padding_mask, src_subsq_mask):\n",
    "\n",
    "        out_cat = None\n",
    "        keys = None\n",
    "        values = None\n",
    "\n",
    "        for i in range(self.num_heads):\n",
    "            head_outp, keys, values = self.heads[i].forward(src, src_padding_mask, src_subsq_mask)\n",
    "\n",
    "            if i == 0:\n",
    "                out_cat = head_outp\n",
    "            else:\n",
    "                out_cat = torch.cat([out_cat, head_outp], dim=2)\n",
    "\n",
    "        ret = self.linear.forward(out_cat)\n",
    "\n",
    "        return ret, keys, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadMemAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.heads = nn.ModuleList([MemAttentionHead(d_model) for i in range(num_heads)])\n",
    "        self.linear = nn.Linear(num_heads * d_model, d_model)\n",
    "\n",
    "    def forward(self, src, src_padding_mask, keys, values):\n",
    "\n",
    "        out_cat = None\n",
    "        for i in range(self.num_heads):\n",
    "            head_outp = self.heads[i].forward(src, src_padding_mask, keys = keys, values = values)\n",
    "\n",
    "            if i == 0:\n",
    "                out_cat = head_outp\n",
    "            else:\n",
    "                out_cat = torch.cat([out_cat, head_outp], dim=2)\n",
    "\n",
    "        ret = self.linear.forward(out_cat)\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_att_heads, ff_dim = 2048, dropout = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multihead_attention = MultiHeadSelfAttention(d_model, num_att_heads)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.att_sublayer_norm = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, ff_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout_lin = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(ff_dim, d_model)\n",
    "\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.lin_sublayer_norm = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src, src_padding_mask, src_subsq_mask):\n",
    "\n",
    "        residual_1 = src\n",
    "        x, keys, values = self.multihead_attention.forward(src, src_padding_mask, src_subsq_mask)\n",
    "        x = self.att_sublayer_norm.forward(residual_1 + self.dropout1(x))\n",
    "\n",
    "        residual_2 = x\n",
    "        x = self.linear2(self.dropout_lin(self.relu(self.linear1.forward(x))))\n",
    "        x = self.lin_sublayer_norm(residual_2 + self.dropout2(x))\n",
    "\n",
    "        return x, keys, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder layer\n",
    "\n",
    "The only difference between Encoder and Decoder layer is the additional MultiHeadMemAttention() sublayer in the decoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_att_heads, ff_dim = 2048, dropout = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multihead_self_attention = MultiHeadSelfAttention(d_model, num_att_heads)\n",
    "        self.self_att_sublayer_norm = torch.nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.multihead_mem_attention = MultiHeadMemAttention(d_model, num_att_heads)\n",
    "        self.mem_att_sublayer_norm = torch.nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, ff_dim)\n",
    "        self.dropout_lin = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(ff_dim, d_model)\n",
    "        self.lin_sublayer_norm = torch.nn.LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_padding_mask, tgt_padding_mask, tgt_subsq_mask, mem_keys, mem_values):\n",
    "\n",
    "        residual_1 = x\n",
    "        x, keys, values = self.multihead_self_attention.forward(x, tgt_padding_mask, tgt_subsq_mask)\n",
    "        x = self.self_att_sublayer_norm.forward(residual_1 + self.dropout1(x))\n",
    "\n",
    "        residual_2 = x\n",
    "        x = self.multihead_mem_attention.forward(x, src_padding_mask, keys = mem_keys, values = mem_values)\n",
    "        x = self.mem_att_sublayer_norm.forward(residual_2 + self.dropout2(x))\n",
    "\n",
    "        residual_3 = x\n",
    "        x = self.linear2(self.dropout_lin(self.relu(self.linear1.forward(x))))\n",
    "        x = self.lin_sublayer_norm(residual_3 + self.dropout3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_att_heads):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_att_heads) for i in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self,src, src_padding_mask, src_subsq_mask):\n",
    "        x = src\n",
    "\n",
    "        keys = None\n",
    "        values = None\n",
    "        for layer in self.layers:\n",
    "            x, keys, values = layer.forward(x, src_padding_mask, src_subsq_mask)\n",
    "\n",
    "        x = self.norm.forward(x)\n",
    "\n",
    "        return keys, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_att_heads):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_att_heads) for i in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, tgt, src_padding_mask, tgt_padding_mask, tgt_subsq_mask, mem_keys, mem_values):\n",
    "        x = tgt\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x, src_padding_mask, tgt_padding_mask, tgt_subsq_mask, mem_keys, mem_values)\n",
    "\n",
    "        x = self.norm.forward(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "The attention module itself have no information about the position of other word embeddings in the sentence. In NLP and other sequential data tasks the order of the sequence is critical. To solve this issue the authors of the paper introduce the Positional Encoding method. First they generate a vector containing information about the position in the same size as the embedding and then simply add this vector to the word embedding and hope that the model will learn to recognize it.\n",
    "\n",
    "They use the following function to generate the positional information: \n",
    "<img src=\"imgs/pos_enc.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.sin_args = torch.zeros(1, self.d_model).to(device)\n",
    "        self.cos_args = torch.zeros(1, self.d_model).to(device)\n",
    "        for i in range(self.d_model//2):\n",
    "            self.sin_args[0,i * 2] = 10000**(2.*i/self.d_model)\n",
    "            self.cos_args[0,i * 2 + 1] = 10000**(2.*i/self.d_model)\n",
    "\n",
    "        self.sin_args_mask = (self.sin_args > 1e-10).float()\n",
    "        self.sin_args = self.sin_args + (self.sin_args < 1e-10).float()\n",
    "\n",
    "        self.cos_args_mask = (self.cos_args > 1e-10).float()\n",
    "        self.cos_args = self.cos_args + (self.cos_args < 1e-10).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for pos in range(x.size()[-2]):\n",
    "            x[:,pos,:] = x[:,pos,:] + \\\n",
    "                         torch.sin(pos / self.sin_args) * self.sin_args_mask + \\\n",
    "                         torch.cos(pos / self.cos_args) * self.cos_args_mask\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a vizualization of the positional encoding vectors. Each row is an encoding for a word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "positional_enc = PositionalEncoding(128).to(device)\n",
    "data = torch.zeros(1, 50, 128).to(device)\n",
    "data_pos_enc = positional_enc.forward(data)\n",
    "\n",
    "enc_np = data_pos_enc.squeeze(dim=0).to('cpu').numpy()\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.imshow(enc_np)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer model\n",
    "\n",
    "The biggest addition to the Transformer module is the translate() function. The translate function reads the source sentence, encodes it with one encoder forward run and then using the **Value** and **Key** matrices from encoder it runs the decoder as many times as needed, generating one additional word per run, until the < end-of-sentence > token is generated or max sentence length is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_att_heads, input_dict_size, output_dict_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_sent_len = 50\n",
    "\n",
    "        self.input_emb = nn.Embedding(input_dict_size, d_model)\n",
    "        self.outp_emb = nn.Embedding(output_dict_size, d_model)\n",
    "\n",
    "        self.positional_encoder = PositionalEncoding(d_model)\n",
    "        self.encoder = Encoder(num_layers, d_model, num_att_heads)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_att_heads)\n",
    "\n",
    "        self.outp_logits = nn.Linear(d_model, output_dict_size)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def translate(self, src, tgt_start_code, tgt_eos_code, src_padding_mask, src_subsq_mask):\n",
    "\n",
    "        # TODO: Beam search\n",
    "\n",
    "        enc_x = self.input_emb.forward(src.squeeze(dim=2))\n",
    "        enc_x = self.positional_encoder.forward(enc_x)\n",
    "        enc_keys, enc_values = self.encoder.forward(enc_x, src_padding_mask, src_subsq_mask)\n",
    "\n",
    "        snt = torch.ones((1,1,1)) * tgt_start_code\n",
    "        snt = snt.long()\n",
    "        snt = snt.to(device)\n",
    "\n",
    "        translation_idxes = []\n",
    "\n",
    "        for idx in range(self.max_sent_len):\n",
    "\n",
    "            dec_x = self.outp_emb.forward(snt.squeeze(dim=2))\n",
    "            dec_x = self.positional_encoder.forward(dec_x)\n",
    "            dec_x = self.decoder.forward(\n",
    "                dec_x,\n",
    "                src_padding_mask = src_padding_mask,\n",
    "                tgt_padding_mask = torch.zeros_like(snt).float().to(device),\n",
    "                tgt_subsq_mask = get_square_subsequent_mask(snt.size()[1]),\n",
    "                mem_keys = enc_keys,\n",
    "                mem_values = enc_values\n",
    "            )\n",
    "            dec_x = self.outp_logits.forward(dec_x)\n",
    "            dec_x = self.softmax(dec_x)\n",
    "            next_word_softmax = dec_x[0,idx,:].to('cpu').detach()\n",
    "            next_word_idx = torch.argmax(next_word_softmax)\n",
    "            snt = torch.cat([snt, torch.ones((1,1,1)).long().to(device) * next_word_idx], dim=1)\n",
    "\n",
    "            translation_idxes.append(next_word_idx)\n",
    "\n",
    "            if next_word_idx == tgt_eos_code:\n",
    "                break\n",
    "\n",
    "        return translation_idxes\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt, src_padding_mask, src_subsq_mask, tgt_padding_mask, tgt_subsq_mask):\n",
    "\n",
    "        enc_x = self.input_emb.forward(src.squeeze(dim=2))\n",
    "        enc_x = self.positional_encoder.forward(enc_x)\n",
    "        enc_keys, enc_values = self.encoder.forward(enc_x, src_padding_mask, src_subsq_mask)\n",
    "\n",
    "        dec_x = self.outp_emb.forward(tgt.squeeze(dim=2))\n",
    "        dec_x = self.positional_encoder.forward(dec_x)\n",
    "        dec_x = self.decoder.forward(dec_x, src_padding_mask, tgt_padding_mask, tgt_subsq_mask, enc_keys, enc_values)\n",
    "        dec_x = self.outp_logits.forward(dec_x)\n",
    "        dec_x = self.softmax(dec_x)\n",
    "\n",
    "        return dec_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_square_subsequent_mask(seq_len):\n",
    "    mask = (torch.triu(torch.ones(seq_len, seq_len).to(device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def get_padding_mask(input, val1 = float('-inf'), val2 = float(0.0)):\n",
    "    mask = torch.ones(input.size()).to(device)\n",
    "    mask = mask.float().masked_fill(input == 0, val1).masked_fill(input > 0, val2)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_one_hot(x, out_dim, mask):\n",
    "\n",
    "    tens = x.view(-1)\n",
    "    tens_one_hot = torch.zeros(list(tens.size()) + [out_dim]).to(device)\n",
    "    for i in range(len(tens)):\n",
    "        tens_one_hot[i,tens[i]] = 1\n",
    "\n",
    "    tens_one_hot = tens_one_hot.view(list(x.size()) + [out_dim])\n",
    "    tens_one_hot = tens_one_hot * mask\n",
    "    return tens_one_hot.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the sentence for model translate function\n",
    "def translate_sentences(src_sentences, tgt_sentences, max_sent_num = 15):\n",
    "\n",
    "    transformer_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for snt_idx in range(len(src_sentences)):\n",
    "\n",
    "            if snt_idx > max_sent_num:\n",
    "                break\n",
    "\n",
    "            src = src_sentences[snt_idx:snt_idx+1]\n",
    "\n",
    "            padded_src = pad_sequence(src, padding_value=0, batch_first=True).to(device)\n",
    "            src_padding_mask = get_padding_mask(padded_src)\n",
    "            src_subsq_mask = get_square_subsequent_mask(padded_src.size()[1])\n",
    "\n",
    "            snt_translation = transformer_model.translate(\n",
    "                src = padded_src,\n",
    "                tgt_start_code = dataset.get_eng_start_code(),\n",
    "                tgt_eos_code = dataset.get_eng_eos_code(),\n",
    "                src_padding_mask = src_padding_mask,\n",
    "                src_subsq_mask = src_subsq_mask\n",
    "            )\n",
    "\n",
    "            src_sent = ''\n",
    "            for word_idx in src_sentences[snt_idx]:\n",
    "                src_sent = f\"{src_sent} {dataset.fra_token_to_text[word_idx]}\"\n",
    "\n",
    "            tgt_sent = ''\n",
    "            for word_idx in tgt_sentences[snt_idx]:\n",
    "                tgt_sent = f\"{tgt_sent} {dataset.eng_token_to_text[word_idx]}\"\n",
    "\n",
    "            translated_sent = ''\n",
    "            for word_idx in snt_translation:\n",
    "                translated_sent = f\"{translated_sent} {dataset.eng_token_to_text[word_idx]}\"\n",
    "\n",
    "            print(f\"Source sentence is: {src_sent}\")\n",
    "            print(f\"Target sentence is: {tgt_sent}\")\n",
    "            print(f\"Model translation is: {translated_sent}\")\n",
    "\n",
    "    transformer_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams, model definition and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170190\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 100\n",
    "STORE_MODELS = True\n",
    "models_path = 'models'\n",
    "\n",
    "if not os.path.exists(models_path):\n",
    "    os.mkdir(models_path)\n",
    "\n",
    "dataset = FraEngDataset()\n",
    "sentences_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=fra_eng_dataset_collate)\n",
    "\n",
    "in_dict_size = dataset.get_fra_dict_size()\n",
    "out_dict_size = dataset.get_eng_dict_size()\n",
    "\n",
    "transformer_model = Transformer(\n",
    "    num_layers=6,\n",
    "    d_model=256,\n",
    "    num_att_heads=8,\n",
    "    input_dict_size=in_dict_size,\n",
    "    output_dict_size=out_dict_size\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr = 1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and sentence translation\n",
    "To follow the porgress of the model closer I will translate few sentenc after every 300 processed batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_count = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    train_loss_sum = 0.0\n",
    "    total_word_count = 0.0\n",
    "\n",
    "    for sentences in sentences_loader:\n",
    "\n",
    "        src_sentences = sentences['fra_sentences']\n",
    "        tgt_sentences = sentences['eng_sentences']\n",
    "\n",
    "        batch_count+=1\n",
    "        if batch_count == 300:\n",
    "            batch_count = 0\n",
    "            translate_sentences(src_sentences,tgt_sentences)\n",
    "            continue\n",
    "\n",
    "        tgt_sentences_out = []\n",
    "\n",
    "        for idx in range(len(tgt_sentences)):\n",
    "            tgt_sentences_out.append(tgt_sentences[idx][1:])\n",
    "            tgt_sentences[idx] = tgt_sentences[idx][:-1]\n",
    "\n",
    "        # Create tensors from token lists\n",
    "        padded_src = pad_sequence(src_sentences, padding_value=0, batch_first=True).to(device)\n",
    "        padded_tgt = pad_sequence(tgt_sentences, padding_value=0, batch_first=True).to(device)\n",
    "        padded_tgt_out = pad_sequence(tgt_sentences_out, padding_value=0, batch_first=True).to(device)\n",
    "\n",
    "        src_padding_mask = get_padding_mask(padded_src)\n",
    "        src_subsq_mask = get_square_subsequent_mask(padded_src.size()[1])\n",
    "\n",
    "        tgt_padding_mask = get_padding_mask(padded_tgt)\n",
    "        tgt_subsq_mask = get_square_subsequent_mask(padded_tgt.size()[1])\n",
    "\n",
    "\n",
    "        pred = transformer_model.forward(\n",
    "            src=padded_src,\n",
    "            tgt=padded_tgt,\n",
    "            src_padding_mask=src_padding_mask,\n",
    "            src_subsq_mask=src_subsq_mask,\n",
    "            tgt_padding_mask=tgt_padding_mask,\n",
    "            tgt_subsq_mask=tgt_subsq_mask\n",
    "        )\n",
    "\n",
    "        # Mask to zero one hot vectors corresponding to padded elements\n",
    "        one_hot_mask = get_padding_mask(padded_tgt_out, val1=float(0.0), val2=float(1.0))\n",
    "        y_one_hot = get_one_hot(padded_tgt_out.squeeze(dim=2), out_dict_size, mask=one_hot_mask)\n",
    "\n",
    "        loss = - torch.sum(torch.log(pred) * y_one_hot)\n",
    "        print(loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss_sum += loss.detach().to('cpu').data\n",
    "        total_word_count += torch.sum(y_one_hot).to('cpu').data\n",
    "\n",
    "    print(f\"Epoch {epoch} \" + '=' * 60)\n",
    "    print(f\"Total loss per word: {train_loss_sum / total_word_count}\")\n",
    "    print(f\"Some translated sentences:\")\n",
    "    \n",
    "    \n",
    "\n",
    "    if STORE_MODELS == True:\n",
    "        model_path = os.path.join(models_path, f'Epoch_{epoch}_model.pt')\n",
    "        torch.save(transformer_model, model_path)\n",
    "\n",
    "print(\"Traing done! Generating some more sentences.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
