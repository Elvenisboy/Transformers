{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning GPT2-medium on a jokes dataset in PyTorch\n",
    "\n",
    "This is an experimental notebook for fine-tuning pre-trained GPT2-medium model on a jokes dataset. Let's see if it can learn to crack some jokes. \n",
    "\n",
    "For this purpose, I will use the pre-trained GPT2 medium size model from huggingface [transformers repository](https://github.com/huggingface/transformers).\n",
    "\n",
    "#### First, check out the *GPT2LMHeadModel* text generation experiments in this [gist](www.com). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select topN() tokens from the probability list(p) and then based on the P(p|top) distribution\n",
    "# select random element\n",
    "def choose_from_top(probs, n=5):\n",
    "    ind = np.argpartition(probs, -n)top[-n:]\n",
    "    top_prob = probs[ind]\n",
    "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
    "    choice = np.random.choice(n, 1, p = top_prob)\n",
    "    token_id = ind[choice][0]\n",
    "    return int(token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset module for Reddit jokes\n",
    "\n",
    "For fine-tuning the GPT2 model, I will use [this](https://github.com/taivop/joke-dataset/blob/master/reddit_jokes.json) jokes dataset. After each joke sample, I add \"<|endofext|>\" which is recognized by the GPT2 model as and end of text marker. The marker will allow me to concatenate many jokes in one sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import json\n",
    "\n",
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, jokes_dataset_path = 'jokes_data/'):\n",
    "        super().__init__()\n",
    "\n",
    "        reddit_jokes_path = os.path.join(jokes_dataset_path, 'reddit_jokes.json')\n",
    "\n",
    "        with open(reddit_jokes_path) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.joke_list = []\n",
    "        self.end_of_text_token = \"<|endoftext|>\"\n",
    "\n",
    "        for idx, joke_json in enumerate(data):\n",
    "            joke_str = f\"{self.end_of_text_token}START:{joke_json['title']} {joke_json['body']}{self.end_of_text_token}\"\n",
    "            self.joke_list.append(joke_str)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.joke_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.joke_list[item]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = JokesDataset()\n",
    "joke_loader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning GPT2-medium on a single GPU\n",
    " \n",
    "Large Transformer models are usually trained in multi-GPU(or TPU) settings because training on reasonable batch size and sequence length requires lots of [tensor|graphical] processing unit memory. My machine is equipped with a single GeForce 1080 Ti, which has 11 GB of memory. By empirical tests on the GPT2 medium model, I found that the maximum total sequence element count in all batches for my GPU to backprop trough is approximately 550, which is not a lot and might not be sufficient for successful fine-tuning. \n",
    "\n",
    "But there are some things we can take into account and improve the situation. \n",
    "\n",
    "The first thing to notice is that batch size in forward/ backward pass of the Transformer does not play a role because [Layer Normalization](https://arxiv.org/abs/1607.06450) is used instead of Batch Normalization. In [Layer Normalization](https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/), each feature is normalized across the feature dimension. \n",
    "\n",
    "Second, we can collect gradients over multiple forward-backward passes, and only then do the model weight update(optimization step). This way, we can store in the memory of the GPU a computational graph of one sequence at a time instead of storing a computational graph of all of the batch. With this strategy, we can get the same result as if the batch would have been processed in a single forward/backward pass, only with *BATCH_SIZE* times less memory.\n",
    "\n",
    "Putting it all together - I will process one sequence at a time with a maximum length of 400. The length of joke sequences varies a lot in the dataset I use, and to make the total sequence element count in one optimization step more consistent, I will try to fit in as many jokes as possible in the 400 element sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "I tested many(I think 5) hyperparameter sets till I found one that works the best. I mostly changed ***BATCH_SIZE*** (in this case, it's the number of forward-backward passes between each optimization step), ***EOPOCHS***, and ***LEARNING_RATE***.\n",
    "\n",
    "For a parameter value starting point for fine-tuning, I inspired from [this](https://github.com/huggingface/transformers/blob/master/examples/run_squad.py) and [this](https://github.com/huggingface/transformers/blob/master/examples/run_glue.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 3e-5\n",
    "WARMUP_STEPS = 10000\n",
    "MAX_SEQ_LEN = 400\n",
    "from transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 started==============================\n",
      "sum loss 3436.03076171875\n",
      "sum loss 3271.525390625\n",
      "sum loss 3117.43603515625\n",
      "sum loss 3020.50732421875\n",
      "sum loss 2940.70654296875\n",
      "sum loss 2915.05078125\n",
      "sum loss 2895.248779296875\n",
      "sum loss 2849.1494140625\n",
      "sum loss 2863.0771484375\n",
      "sum loss 2827.261474609375\n",
      "sum loss 2824.12109375\n",
      "sum loss 2795.527587890625\n",
      "sum loss 2803.104248046875\n",
      "sum loss 2802.185791015625\n",
      "sum loss 2786.28515625\n",
      "sum loss 2778.56982421875\n",
      "sum loss 2762.30615234375\n",
      "sum loss 2770.957763671875\n",
      "sum loss 2754.240478515625\n",
      "sum loss 2747.343994140625\n",
      "sum loss 2726.651611328125\n",
      "sum loss 2729.69189453125\n",
      "sum loss 2744.18212890625\n",
      "sum loss 2724.10400390625\n",
      "sum loss 2714.15283203125\n",
      "sum loss 2711.803955078125\n",
      "sum loss 2703.7119140625\n",
      "sum loss 2704.89208984375\n",
      "sum loss 2697.152099609375\n",
      "sum loss 2689.19189453125\n",
      "sum loss 2671.143798828125\n",
      "sum loss 2677.786376953125\n",
      "EPOCH 1 started==============================\n",
      "sum loss 2645.66064453125\n",
      "sum loss 2622.15966796875\n",
      "sum loss 2662.0419921875\n",
      "sum loss 2641.111083984375\n",
      "sum loss 2650.60546875\n",
      "sum loss 2637.7275390625\n",
      "sum loss 2634.77392578125\n",
      "sum loss 2637.45947265625\n",
      "sum loss 2636.165771484375\n",
      "sum loss 2640.04833984375\n",
      "sum loss 2617.153564453125\n",
      "sum loss 2619.0419921875\n",
      "sum loss 2604.137451171875\n",
      "sum loss 2599.365966796875\n",
      "sum loss 2615.25048828125\n",
      "sum loss 2592.6669921875\n",
      "sum loss 2599.585693359375\n",
      "sum loss 2598.38671875\n",
      "sum loss 2583.61376953125\n",
      "sum loss 2582.79296875\n",
      "sum loss 2598.868408203125\n",
      "sum loss 2579.271728515625\n",
      "sum loss 2582.907470703125\n",
      "sum loss 2582.181396484375\n",
      "sum loss 2595.12353515625\n",
      "sum loss 2563.3701171875\n",
      "sum loss 2551.559814453125\n",
      "sum loss 2555.54833984375\n",
      "sum loss 2581.083740234375\n",
      "sum loss 2553.90234375\n",
      "sum loss 2576.71240234375\n",
      "sum loss 2559.1435546875\n",
      "EPOCH 2 started==============================\n",
      "sum loss 2484.018798828125\n",
      "sum loss 2495.583984375\n",
      "sum loss 2504.78466796875\n",
      "sum loss 2515.67822265625\n",
      "sum loss 2489.676025390625\n",
      "sum loss 2484.712890625\n",
      "sum loss 2504.12109375\n",
      "sum loss 2473.819580078125\n",
      "sum loss 2479.34326171875\n",
      "sum loss 2486.447265625\n",
      "sum loss 2476.236328125\n",
      "sum loss 2456.914794921875\n",
      "sum loss 2465.84375\n",
      "sum loss 2499.381591796875\n",
      "sum loss 2472.593994140625\n",
      "sum loss 2437.755126953125\n",
      "sum loss 2472.271728515625\n",
      "sum loss 2451.429931640625\n",
      "sum loss 2459.23291015625\n",
      "sum loss 2483.620361328125\n",
      "sum loss 2445.205322265625\n",
      "sum loss 2467.73095703125\n",
      "sum loss 2455.337158203125\n",
      "sum loss 2473.5849609375\n",
      "sum loss 2487.701416015625\n",
      "sum loss 2458.47509765625\n",
      "sum loss 2455.560302734375\n",
      "sum loss 2471.487060546875\n",
      "sum loss 2481.9384765625\n",
      "sum loss 2469.49072265625\n",
      "sum loss 2475.31884765625\n",
      "sum loss 2467.7421875\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=WARMUP_STEPS, t_total = -1)\n",
    "proc_seq_count = 0\n",
    "sum_loss = 0.0\n",
    "batch_count = 0\n",
    "\n",
    "tmp_jokes_tens = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    print(f\"EPOCH {epoch} started\" + '=' * 30)\n",
    "    \n",
    "    for idx,joke in enumerate(joke_loader):\n",
    "        \n",
    "        #################### \"Fit as many joke sequences into MAX_SEQ_LEN sequence as possible\" logic start ####\n",
    "        joke_tens = torch.tensor(tokenizer.encode(joke[0])).unsqueeze(0).to(device)\n",
    "        #Skip sample from dataset if it is longer than MAX_SEQ_LEN\n",
    "        if joke_tens.size()[1] > MAX_SEQ_LEN:\n",
    "            continue\n",
    "        \n",
    "        #The first joke sequence in the sequence\n",
    "        if not torch.is_tensor(tmp_jokes_tens):\n",
    "            tmp_jokes_tens = joke_tens\n",
    "            continue\n",
    "        else:\n",
    "            #The next joke does not fit in so we process the sequence and leave the last joke \n",
    "            #as the start for next sequence \n",
    "            if tmp_jokes_tens.size()[1] + joke_tens.size()[1] > MAX_SEQ_LEN:\n",
    "                work_jokes_tens = tmp_jokes_tens\n",
    "                tmp_jokes_tens = joke_tens\n",
    "            else:\n",
    "                #Add the joke to sequence, continue and try to add more\n",
    "                tmp_jokes_tens = torch.cat([tmp_jokes_tens, joke_tens[:,1:]], dim=1)\n",
    "                continue\n",
    "        ################## Sequence ready, process it trough the model ##################\n",
    "            \n",
    "        outputs = model(work_jokes_tens, labels=work_jokes_tens)\n",
    "        loss, logits = outputs[:2]                        \n",
    "        loss.backward()\n",
    "        sum_loss = sum_loss + loss.detach().data\n",
    "                       \n",
    "        proc_seq_count = proc_seq_count + 1\n",
    "        if proc_seq_count == BATCH_SIZE:\n",
    "            proc_seq_count = 0    \n",
    "            batch_count += 1\n",
    "            optimizer.step()\n",
    "            scheduler.step() \n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            \n",
    "        if batch_count == 1000:\n",
    "            print(f\"sum loss {sum_loss}\")\n",
    "            batch_count = 0\n",
    "            sum_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating some jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for joke_idx in range(500):\n",
    "\n",
    "        cur_ids = torch.tensor(tokenizer.encode(\"<|endoftext|>START:\")).unsqueeze(0).to(device)\n",
    "        \n",
    "        for i in range(250):\n",
    "            outputs = model(cur_ids, labels=cur_ids)\n",
    "            loss, logits = outputs[:2]\n",
    "            softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(only one in this case) batch and the last predicted embedding\n",
    "            if i < 2:\n",
    "                n = 15\n",
    "            else:\n",
    "                n = 3\n",
    "            next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n) #Randomly(from the topN probability distribution) choose the next word from the top n words\n",
    "            cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word\n",
    "\n",
    "            if next_token_id in tokenizer.encode('<|endoftext|>'):\n",
    "                break\n",
    "            \n",
    "        output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
    "        output_text = tokenizer.decode(output_list)\n",
    "        print(f\"JOKE NR {joke_idx}: {output_text} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output was too long and I stored it in [this file](github.link)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(\"gpt2_medium_joker.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load stored model to generate some more jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"gpt2_medium_joker.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
