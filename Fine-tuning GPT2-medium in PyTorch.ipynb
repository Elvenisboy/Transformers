{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning GPT-2 on a jokes dataset in PyTorch\n",
    "\n",
    "This notebook was created as a part of a blog post - [Fine-tuning large Transformer models on a single GPU in PyTorch - Teaching GPT-2 a sense of humor](www). Here I demonstrate how to fine-tune a pre-trained GPT-2 model on a jokes dataset. \n",
    "\n",
    "Let's see if the model can learn to crack some jokes!\n",
    "\n",
    "For this experiment, I will use a pre-trained GPT-2 medium-sized model from the huggingface [transformers repository](https://github.com/huggingface/transformers).\n",
    "\n",
    "#### If you haven't yet, check out the notebook in this [gist](https://gist.github.com/mf1024/430d7fd6ff527350d3e4b5bda0d8614e) where you will find some more details about setting up and using the pre-trained model for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1213 07:29:13.930225 140680729618176 file_utils.py:32] TensorFlow version 2.0.0 available.\n",
      "I1213 07:29:13.930676 140680729618176 file_utils.py:39] PyTorch version 1.3.0 available.\n",
      "I1213 07:29:14.147772 140680729618176 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_from_top(probs, n=5):\n",
    "    ind = np.argpartition(probs, -n)[-n:]\n",
    "    top_prob = probs[ind]\n",
    "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
    "    choice = np.random.choice(n, 1, p = top_prob)\n",
    "    token_id = ind[choice][0]\n",
    "    return int(token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset module for Reddit jokes\n",
    "\n",
    "For fine-tuning the GPT2 model, I will use Reddit jokes from [this](https://github.com/taivop/joke-dataset/blob/master/reddit_jokes.json) dataset. After each joke sample, I add \"<|endofext|>\" which is recognized by the GPT2 model as and end of text marker. The marker will allow me to concatenate many jokes in a single sequence input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import json\n",
    "\n",
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, jokes_dataset_path = 'jokes_data/'):\n",
    "        super().__init__()\n",
    "\n",
    "        reddit_jokes_path = os.path.join(jokes_dataset_path, 'reddit_jokes.json')\n",
    "\n",
    "        with open(reddit_jokes_path) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.joke_list = []\n",
    "        self.end_of_text_token = \"<|endoftext|>\"\n",
    "\n",
    "        for idx, joke_json in enumerate(data):\n",
    "            joke_str = f\"{self.end_of_text_token}START:{joke_json['title']} {joke_json['body']}{self.end_of_text_token}\"\n",
    "            self.joke_list.append(joke_str)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.joke_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.joke_list[item]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = JokesDataset()\n",
    "joke_loader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "I tested many(I think 5) hyperparameter sets till I found one that works the best. I mostly tuned ***BATCH_SIZE*** (in this case, it's the number of forward-backward passes between each optimization step), ***EOPOCHS***, and ***LEARNING_RATE***.\n",
    "\n",
    "For a parameter value starting point for fine-tuning, I inspired from [this](https://github.com/huggingface/transformers/blob/master/examples/run_squad.py) and [this](https://github.com/huggingface/transformers/blob/master/examples/run_glue.py) piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 3e-5\n",
    "WARMUP_STEPS = 10000\n",
    "MAX_SEQ_LEN = 400\n",
    "from transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 started==============================\n",
      "sum loss 3436.03076171875\n",
      "sum loss 3271.525390625\n",
      "sum loss 3117.43603515625\n",
      "sum loss 3020.50732421875\n",
      "sum loss 2940.70654296875\n",
      "sum loss 2915.05078125\n",
      "sum loss 2895.248779296875\n",
      "sum loss 2849.1494140625\n",
      "sum loss 2863.0771484375\n",
      "sum loss 2827.261474609375\n",
      "sum loss 2824.12109375\n",
      "sum loss 2795.527587890625\n",
      "sum loss 2803.104248046875\n",
      "sum loss 2802.185791015625\n",
      "sum loss 2786.28515625\n",
      "sum loss 2778.56982421875\n",
      "sum loss 2762.30615234375\n",
      "sum loss 2770.957763671875\n",
      "sum loss 2754.240478515625\n",
      "sum loss 2747.343994140625\n",
      "sum loss 2726.651611328125\n",
      "sum loss 2729.69189453125\n",
      "sum loss 2744.18212890625\n",
      "sum loss 2724.10400390625\n",
      "sum loss 2714.15283203125\n",
      "sum loss 2711.803955078125\n",
      "sum loss 2703.7119140625\n",
      "sum loss 2704.89208984375\n",
      "sum loss 2697.152099609375\n",
      "sum loss 2689.19189453125\n",
      "sum loss 2671.143798828125\n",
      "sum loss 2677.786376953125\n",
      "EPOCH 1 started==============================\n",
      "sum loss 2645.66064453125\n",
      "sum loss 2622.15966796875\n",
      "sum loss 2662.0419921875\n",
      "sum loss 2641.111083984375\n",
      "sum loss 2650.60546875\n",
      "sum loss 2637.7275390625\n",
      "sum loss 2634.77392578125\n",
      "sum loss 2637.45947265625\n",
      "sum loss 2636.165771484375\n",
      "sum loss 2640.04833984375\n",
      "sum loss 2617.153564453125\n",
      "sum loss 2619.0419921875\n",
      "sum loss 2604.137451171875\n",
      "sum loss 2599.365966796875\n",
      "sum loss 2615.25048828125\n",
      "sum loss 2592.6669921875\n",
      "sum loss 2599.585693359375\n",
      "sum loss 2598.38671875\n",
      "sum loss 2583.61376953125\n",
      "sum loss 2582.79296875\n",
      "sum loss 2598.868408203125\n",
      "sum loss 2579.271728515625\n",
      "sum loss 2582.907470703125\n",
      "sum loss 2582.181396484375\n",
      "sum loss 2595.12353515625\n",
      "sum loss 2563.3701171875\n",
      "sum loss 2551.559814453125\n",
      "sum loss 2555.54833984375\n",
      "sum loss 2581.083740234375\n",
      "sum loss 2553.90234375\n",
      "sum loss 2576.71240234375\n",
      "sum loss 2559.1435546875\n",
      "EPOCH 2 started==============================\n",
      "sum loss 2484.018798828125\n",
      "sum loss 2495.583984375\n",
      "sum loss 2504.78466796875\n",
      "sum loss 2515.67822265625\n",
      "sum loss 2489.676025390625\n",
      "sum loss 2484.712890625\n",
      "sum loss 2504.12109375\n",
      "sum loss 2473.819580078125\n",
      "sum loss 2479.34326171875\n",
      "sum loss 2486.447265625\n",
      "sum loss 2476.236328125\n",
      "sum loss 2456.914794921875\n",
      "sum loss 2465.84375\n",
      "sum loss 2499.381591796875\n",
      "sum loss 2472.593994140625\n",
      "sum loss 2437.755126953125\n",
      "sum loss 2472.271728515625\n",
      "sum loss 2451.429931640625\n",
      "sum loss 2459.23291015625\n",
      "sum loss 2483.620361328125\n",
      "sum loss 2445.205322265625\n",
      "sum loss 2467.73095703125\n",
      "sum loss 2455.337158203125\n",
      "sum loss 2473.5849609375\n",
      "sum loss 2487.701416015625\n",
      "sum loss 2458.47509765625\n",
      "sum loss 2455.560302734375\n",
      "sum loss 2471.487060546875\n",
      "sum loss 2481.9384765625\n",
      "sum loss 2469.49072265625\n",
      "sum loss 2475.31884765625\n",
      "sum loss 2467.7421875\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=WARMUP_STEPS, t_total = -1)\n",
    "proc_seq_count = 0\n",
    "sum_loss = 0.0\n",
    "batch_count = 0\n",
    "\n",
    "tmp_jokes_tens = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    print(f\"EPOCH {epoch} started\" + '=' * 30)\n",
    "    \n",
    "    for idx,joke in enumerate(joke_loader):\n",
    "        \n",
    "        #################### \"Fit as many joke sequences into MAX_SEQ_LEN sequence as possible\" logic start ####\n",
    "        joke_tens = torch.tensor(tokenizer.encode(joke[0])).unsqueeze(0).to(device)\n",
    "        #Skip sample from dataset if it is longer than MAX_SEQ_LEN\n",
    "        if joke_tens.size()[1] > MAX_SEQ_LEN:\n",
    "            continue\n",
    "        \n",
    "        #The first joke sequence in the sequence\n",
    "        if not torch.is_tensor(tmp_jokes_tens):\n",
    "            tmp_jokes_tens = joke_tens\n",
    "            continue\n",
    "        else:\n",
    "            #The next joke does not fit in so we process the sequence and leave the last joke \n",
    "            #as the start for next sequence \n",
    "            if tmp_jokes_tens.size()[1] + joke_tens.size()[1] > MAX_SEQ_LEN:\n",
    "                work_jokes_tens = tmp_jokes_tens\n",
    "                tmp_jokes_tens = joke_tens\n",
    "            else:\n",
    "                #Add the joke to sequence, continue and try to add more\n",
    "                tmp_jokes_tens = torch.cat([tmp_jokes_tens, joke_tens[:,1:]], dim=1)\n",
    "                continue\n",
    "        ################## Sequence ready, process it trough the model ##################\n",
    "            \n",
    "        outputs = model(work_jokes_tens, labels=work_jokes_tens)\n",
    "        loss, logits = outputs[:2]                        \n",
    "        loss.backward()\n",
    "        sum_loss = sum_loss + loss.detach().data\n",
    "                       \n",
    "        proc_seq_count = proc_seq_count + 1\n",
    "        if proc_seq_count == BATCH_SIZE:\n",
    "            proc_seq_count = 0    \n",
    "            batch_count += 1\n",
    "            optimizer.step()\n",
    "            scheduler.step() \n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            \n",
    "        if batch_count == 1000:\n",
    "            print(f\"sum loss {sum_loss}\")\n",
    "            batch_count = 0\n",
    "            sum_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating some jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for joke_idx in range(500):\n",
    "\n",
    "        cur_ids = torch.tensor(tokenizer.encode(\"<|endoftext|>START:\")).unsqueeze(0).to(device)\n",
    "        \n",
    "        for i in range(250):\n",
    "            outputs = model(cur_ids, labels=cur_ids)\n",
    "            loss, logits = outputs[:2]\n",
    "            softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(only one in this case) batch and the last predicted embedding\n",
    "            if i < 2:\n",
    "                n = 15\n",
    "            else:\n",
    "                n = 3\n",
    "            next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n) #Randomly(from the topN probability distribution) choose the next word from the top n words\n",
    "            cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word\n",
    "\n",
    "            if next_token_id in tokenizer.encode('<|endoftext|>'):\n",
    "                break\n",
    "            \n",
    "        output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
    "        output_text = tokenizer.decode(output_list)\n",
    "        print(f\"JOKE NR {joke_idx}: {output_text} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output was too long, so I stored it in [this file](github.link)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(\"gpt2_medium_joker.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load stored model to generate more jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"gpt2_medium_joker.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
